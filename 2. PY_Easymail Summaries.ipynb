{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. PY_Easymail Summaries.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMbaTj2AQowbxj76rR1AWYz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"1J3cOwHIGHVW"},"outputs":[],"source":["# Mount GDrive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"code","source":["import pandas as pd\n","\n","file_path = \"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/\"\n","file_name = \"enron_unique_1000_NER\"\n","file = file_path+file_name+'.csv'\n","\n","df_emails = pd.read_csv(file)\n","df_emails.tail(10)\n","deb = 0\n","fin = df_emails.shape[0]"],"metadata":{"id":"Z9_TLtESa9Cs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# SAUVEGARDE en csv du résultat.\n","def svgd_csv_xls (file_path,file_name):\n","\n","# to .CSV\n","  file_extension = \".csv\"\n","  file = file_path+file_name+file_extension\n","  print(\"File : \",file)\n","  df_emails.to_csv(file, encoding='utf-8', index=False)\n","# df_emails[deb:fin].to_csv(file, encoding='utf-8', index=False)\n","  print(\"Svgde effectuée\")\n","  return\n"],"metadata":{"id":"hFvRiVNLbEnZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fonctions Summary methods"],"metadata":{"id":"J2JLk0qadCNj"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","def TF(token, artikle):\n","    \"\"\"\n","    Calcule le score TF d'un mot dans un artikle\n","    \n","    token : Mot dont le score TF doit être calculé.\n","    \n","    artikle : Dictionnaire généré à partir d'un texte.\n","    \"\"\"\n","    f = 0\n","    for key in artikle:\n","        for work in artikle[key]:\n","            if work == token:\n","                f += 1 \n","    return np.log(f+1)\n","\n","def IDF(token, corpus):\n","    \"\"\"\n","    Calcule le score IDF d'un mot dans un corpus d'artikles.\n","    \n","    token : Mot dont le score IDF doit être calculé.\n","    \n","    corpus : Liste d'artikles.\n","    \"\"\"\n","    N = len(corpus)\n","    d=0\n","    present = False\n","    \n","    for artikle in corpus:\n","        for key in artikle:\n","            if token in artikle[key]:\n","                present = True\n","        d += int(present)\n","        present = False\n","                \n","    return np.log(N/(d+1) +1)\n","\n","def TFIDF(token, artikle, corpus):\n","    \"\"\"\n","    Calcule le score TF-IDF d'un mot dans un texte.\n","    \n","    token : mot dont le score doit être calculé.\n","    \n","    artikle : artikle qui servira à calculer le score du mot dans cet artikle.\n","    \n","    corpus : Liste d'artikles formant le corpus.\n","    \"\"\"\n","    return TF(token, artikle)*IDF(token, corpus)\n","\n","def score_sentence(corpus, artikle, sentence):\n","    \"\"\"\n","    Calcule le score d'une phrase.\n","    \n","    corpus : Liste d'artikles.\n","    \n","    artikle : Dictionnaire de phrases.\n","    \n","    sentence : Phrase sous forme de liste de mots.\n","    \"\"\"\n","    score_sentence = []\n","    for word in sentence :\n","        score_word = TFIDF(word, artikle, corpus)\n","        score_sentence.append(score_word)\n","    return np.mean(score_sentence)\n","\n","def best_sentences(scores_artikle, nb_sentences):\n","    \"\"\"\n","    Retourne les indices des phrases les plus importantes en fonction des scores obtenus.\n","    \n","    scores_artikle : Liste des scores de chaque phrase dans un texte.\n","    \n","    nb_sentences : Nombre de phrases à sélectionner.\n","    \"\"\"\n","    \n","    return sorted(np.argsort(scores_artikle)[-nb_sentences:])\n","\n","def summarize_TFIDF(i, n_sentences, df):\n","    \"\"\"\n","    Synthèse extractive d'un article par la méthode TF-IDF.\n","    \n","    i : indice de l'article dans le corpus.\n","    \n","    n_sentences : nombre de phrases à conserver.\n","    \n","    df : DataFrame contenant les artikles dans une colonne 'Artikle'.    \n","    \"\"\"\n","    corpus = df['body_dict']\n","    artikle = corpus[i]\n","    texte=df['body'][i]\n","    \n","    if len(corpus) <= n_sentences:\n","      print(\"Longeur corpus inférieure au nombre minimal de phrases retenu pour le résumé\")\n","      texte = \"See Original\"\n","      return texte\n","    # Calcul du score de chaque phrase de l'artikle\n","    scores_artikle = [score_sentence(corpus, artikle, sentence) for sentence in artikle.values()]\n"," \n","    \n","    # Extraction des indices des phrases ayant les meilleurs scores\n","    result = best_sentences(scores_artikle, n_sentences)\n","#    print('best sentences :',result)\n","    \n","    # Séparation de phrases l'article original \n","    tokenized_article = sent_tokenize(texte)\n","    \n","    # Extraction des phrases les plus importantes\n","    summary = [tokenized_article[i] for i in result]\n","    \n","    # transformation finale en chaine de caracteres\n","    texte = ''.join(summary)\n","    return texte\n","\n","def summarize_spacy (text,ratio):\n","  # https://www.numpyninja.com/post/text-summarization-through-use-of-spacy-library\n","  # print('\\n Texte à résumer avec un ratio de ',ratio*100,\"%\")\n","  # print(text)\n","  nlp = spacy.load('en_core_web_lg')\n","  doc= nlp(text)\n","  tokens=[token.text for token in doc]\n","#  print(\"Tokens : \\n\",tokens)\n","\n","  # calcul frequence de mots\n","  word_frequencies={}\n","  for word in doc:\n","    if word.text.lower() not in stopwords:\n","      if word.text.lower() not in punctuation:\n","        if word.text not in word_frequencies.keys():\n","          word_frequencies[word.text] = 1\n","        else:\n","          word_frequencies[word.text] += 1\n","  # print(\"\\n word_frequencies : \",word_frequencies)\n","\n","  # normalisation des frequence de mots\n","  max_frequency=max(word_frequencies.values())\n","  # print('Max Frequency : ',max_frequency)\n","  for word in word_frequencies.keys():\n","    word_frequencies[word]=word_frequencies[word]/max_frequency\n","  # print(\"\\n Normalized word_frequencies : \\n\",word_frequencies)\n","  # Sentences token\n","  sentence_tokens= [sent for sent in doc.sents]\n","  # print(\"sentence_tokens \",sentence_tokens)\n","  # Calculate the most important sentences by adding the word frequencies in each sentence.\n","  sentence_scores = {}\n","  for sent in sentence_tokens:\n","    for word in sent:\n","      if word.text.lower() in word_frequencies.keys():\n","        if sent not in sentence_scores.keys():                            \n","          sentence_scores[sent]=word_frequencies[word.text.lower()]\n","        else:\n","          sentence_scores[sent]+=word_frequencies[word.text.lower()]\n","\n","  # identifier % (ratio) du texte avec score maximum\n","  from heapq import nlargest\n","  select_length=int(len(sentence_tokens)*ratio)\n","  # print('select_length ',select_length)\n","  if select_length != 0:\n","    summary=nlargest(select_length, sentence_scores,key=sentence_scores.get)\n","    final_summary=[word.text for word in summary]\n","    summary=''.join(final_summary)\n","  else:\n","    summary = 'See Original'\n","  return summary\n"],"metadata":{"id":"XUUnpRWscs-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets transformers[sentencepiece]"],"metadata":{"id":"nOT4Wr2KdpRe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# T5 (non performant)\n","from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n","my_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n","tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","\n","def summarize_T5 (text):\n","  text = \"summarize:\" + text\n","  # encoding the input text\n","  input_ids=tokenizer.encode(text, return_tensors='pt', max_length=512)\n","  # Generating summary ids\n","  summary_ids = my_model.generate(input_ids)\n","  # Decoding the tensor and printing the summary.\n","  t5_summary = tokenizer.decode(summary_ids[0])\n","  return t5_summary"],"metadata":{"id":"JVinmBp3dJNf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n","# Loading the model and tokenizer for bart-large-cnn\n","tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","\n","def summarize_BART (text):\n","  # Encoding the inputs and passing them to model.generate()\n","  inputs = tokenizer.batch_encode_plus([text],return_tensors='pt')\n","  summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n","  # Decoding and printing the summary\n","  bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","  # print('bart_summary',bart_summary)\n","  return bart_summary"],"metadata":{"id":"zBFkFb3ddJnW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["TF-IDF"],"metadata":{"id":"o35_AtDzd-Yz"}},{"cell_type":"code","source":["\n","print(\"Traitement TF-IDF en cours pour \",fin, \"records.\")\n","for id in range(deb,fin):\n","  print(\" id : \",id)\n","  text = df_emails['body_clean'][id]\n","  # Extractive summary using TF-IDF\n","  # ------------------------------------------------------------------------\n","  # suppression de stopwords pour TFIDF\n","  text = remove_stop_words(text,stopwords)\n","\n","  # Dictionnarize the text to be used with TF-IDF\n","  df_emails['body_dict'][id] = dictionarize(text)   # sans stopwords\n","  max_sent = 3 \n","  if len(df_emails['body_dict'][id]) > max_sent:\n","    text = df_emails['body_dict'][id]\n","    # on affiche phrase par phrase le contenu de chaque mail\n","    # for cle, valeur in text.items():\n","    #   print(\"Ligne \", cle, \" : \", valeur)\n","    # Identification des n_sent les plus significatives\n","\n","    df_emails['summary_TFIDF'][id] = summarize_TFIDF(id, max_sent, df_emails)\n","    ratio = len(df_emails['summary_TFIDF'][id])/len(df_emails['body_clean'][id])\n","    # print(\"\\nRésumé TFIDF - Len \",len(df_emails['summary_TFIDF'][id]),\" Ratio : \",int(ratio*100),\"%\")\n","    # print(df_emails['summary_TFIDF'][id])\n","  else:\n","    # print(\"ID \",id,\" contenu trop court. Reprise de l'original pour TFIDF\")\n","    df_emails['summary_TFIDF'][id] = df_emails['body_clean'][id]\n","\n","\n","print(\"Traitement TF-IDF terminé pour \",fin, \"records.\")\n","svgd_csv_xls (file_path,file_name)"],"metadata":{"id":"_yKYoVbudJ6s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Spacy"],"metadata":{"id":"8kDMkc_5eJvX"}},{"cell_type":"code","source":["print(\"Résumé par Spacy en cours pour \",fin, \"records.\")\n","for id in range(deb,fin):\n","  print(\" id : \",id)\n","# ------------------------------------------------------------------------\n","  # Extractive summary using Spacy\n","  ratio = 0.20\n","  text = df_emails['body_clean'].iloc[id]\n","  df_emails['summary_spacy'][id] = summarize_spacy (str(text),ratio)\n","  ratio = len(df_emails['summary_spacy'][id])/len(df_emails['body_clean'][id])\n","  # print(\"\\nRésumé Spacy - Len \",len(df_emails['summary_spacy'][id]),\"Ratio : \",int(ratio*100),\"%\")\n","  # print(df_emails['summary_spacy'][id])\n","\n","print(\"Résumé par Spacy terminé pour \",fin, \"records.\")\n","svgd_csv_xls (file_path,file_name)"],"metadata":{"id":"mjV_aZQqeBIO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["BART"],"metadata":{"id":"ytE1RqEeeUw_"}},{"cell_type":"code","source":["print(\"Traitement résume avec BART en cours pour \",fin, \"records.\")\n","for id in range(deb,fin):\n","  print(\"id : \",id)\n","  # print('--------------------')\n","  # coller quand OK pour les tests unitaires\n","  df_emails['body_clean'][id] = data_clean(str(df_emails['body'].iloc[id]))\n","  text = df_emails['body_clean'][id]\n","  # print(\"Cleaned text : \",len(str(text)),\" car.\\n\")\n","  # print(text)\n"," \n","# ------------------------------------------------------------------------\n","# Abstractive summary using Transformers BART\n","  # print(\"\\nExtractive summary using Transformers / BART\")\n","  text = df_emails['body_clean'].iloc[id]\n","  # print('text :',text)\n","  try:\n","    df_emails['summary_BART'][id] = summarize_BART (text)\n","    ratio = len(df_emails['summary_BART'][id])/len(df_emails['body_clean'][id])\n","    # print(\"\\nRésumé Transformers BART  - Len \",len(df_emails['summary_BART'][id]),\" Ratio : \",int(ratio*100),\"%\")\n","  except:\n","    # print(\"Pb extractive summary on Id \",id)\n","    df_emails['summary_BART'][id] = \"Erreur\"\n","\n","  # Svgd version intermédiaire\n","  if int(id/100) == id/100:\n","    file_name_tmp = \"enron_unique_output_\"+str(id)\n","    file = file_path+file_name_tmp\n","    svgd_csv_xls (file_path,file_name_tmp)\n","    print(file_name_tmp,\" sauvegardé.\")\n","\n","print(\"Traitement résume avec BART terminé pour \",fin, \"records.\")\n","\n","svgd_csv_xls (file_path,file_name)"],"metadata":{"id":"xK_bPJXueBbk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Calcul similarités des résumés"],"metadata":{"id":"H2ZAYkGQegjJ"}},{"cell_type":"code","source":["spacy.load('en_core_web_lg')\n","nlp = spacy.load('en_core_web_lg')"],"metadata":{"id":"xKKT2Mw3eBsG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Traitement en cours pour \",fin, \"records\")\n","\n","for id in range(deb,fin):\n","  print(\"ID:\",id)\n","  # Calcul de similarité avec texte initial nettoyé\n","  doc1 = nlp(df_emails['body_clean'][id])\n","  max_sim = 0\n","  doc2 = nlp(df_emails['summary_TFIDF'][id])\n","  if doc2 != \"\":\n","    sim = doc1.similarity(doc2)\n","    df_emails['summary_TFIDF_sim'][id] = sim\n","    max_sim = sim\n","\n","  doc2 = nlp(df_emails['summary_spacy'][id])\n","  if doc2 != \"\":\n","    sim = doc1.similarity(doc2)\n","    df_emails['summary_spacy_sim'][id] = sim\n","    if sim > max_sim:\n","      max_sim = sim\n","\n","  doc2 = nlp(df_emails['summary_T5'][id])\n","  if doc2 != \"Erreur\":\n","    sim = doc1.similarity(doc2)\n","    df_emails['summary_T5_sim'][id] = sim\n","    if sim > max_sim:\n","      max_sim = sim\n","\n","  doc2 = nlp(df_emails['summary_BART'][id])\n","  if doc2 != \"Erreur\":\n","    sim = doc1.similarity(doc2)\n","    df_emails['summary_BART_sim'][id] = sim\n","    if sim > max_sim:\n","      max_sim = sim\n","\n","  # print(\"Meilleur score : \",max_sim)\n","  df_emails['best_sim'][id] = max_sim\n","\n","\n","svgd_csv_xls (file_path,file_name)"],"metadata":{"id":"0BREfRy0ec09"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"dwe9W7POedFW"},"execution_count":null,"outputs":[]}]}