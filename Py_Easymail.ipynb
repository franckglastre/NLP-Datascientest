{"cells":[{"cell_type":"markdown","metadata":{"id":"-ZKemb11VlJD"},"source":["# Structuration DataSet (A ne plus relancer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8u5xskFVyMI","executionInfo":{"status":"ok","timestamp":1655902578547,"user_tz":-120,"elapsed":28046,"user":{"displayName":"Nico T","userId":"09615745923254077330"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"bebd3da5-9721-485f-c5b5-6798b6917ffa"},"outputs":[{"output_type":"stream","name":"stdout","text":["(405968, 6)\n","(227032, 5)\n"]}],"source":["# Mount GDrive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# On concaténe les 6 fichiers\n","import pandas as pd\n","df_0 = pd.read_csv(\"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/emaildata_100000_0.csv\")\n","df_1 = pd.read_csv(\"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/emaildata_100000_1.csv\")\n","df_2 = pd.read_csv(\"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/emaildata_100000_2.csv\")\n","df_3 = pd.read_csv(\"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/emaildata_100000_3.csv\")\n","df_4 = pd.read_csv(\"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/emaildata_100000_4.csv\")\n","df_5 = pd.read_csv(\"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/emaildata_100000_5.csv\")\n","# df = pd.read_csv(\"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/enron.csv\", nrows=1500)\n","frames = [df_0,df_1,df_2,df_3,df_4,df_5]\n","df = pd.concat(frames)\n","# la selection ci-dessous a permis de détecter 3 doublons (toutes colonnes identiques)\n","# df = df.loc[(df['subject'] == \"Sagewood Town Homes\")]\n","print(df.shape)\n","# df = df_emails.loc[(df['subject'] == \"Sagewood Town Homes\")]\n","df.drop('Unnamed: 0', axis=1, inplace=True)\n","# suppression des doublons (sur toutes les colonnes sauf index)\n","df.drop_duplicates(subset=['date','sender', 'recipient1', 'subject','text'],keep = 'first', inplace=True)\n","# on renomme les colonnes\n","df = df.rename(columns={'sender': 'from', 'recipient1': 'to','subject': 'header', 'text': 'body'})\n","# et export en csv du résultat.\n","file_name = \"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/enron_unique.csv\"\n","df.to_csv(file_name, encoding='utf-8', index=False)\n","print(df.shape)\n","# Taille du fichier divisée par 2 environ"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1655902578548,"user":{"displayName":"Nico T","userId":"09615745923254077330"},"user_tz":-120},"id":"YP8TOi7VWtOs","outputId":"a12d4ace-5499-432f-ab08-caf9ae7dc2e9"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 227032 entries, 0 to 11736\n","Data columns (total 5 columns):\n"," #   Column  Non-Null Count   Dtype \n","---  ------  --------------   ----- \n"," 0   date    227032 non-null  object\n"," 1   from    227032 non-null  object\n"," 2   to      227032 non-null  object\n"," 3   header  218946 non-null  object\n"," 4   body    227032 non-null  object\n","dtypes: object(5)\n","memory usage: 10.4+ MB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","source":["# Data Preprocessing - cleaning"],"metadata":{"id":"AQ7Tk9bJn1Gw"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"_giUgOlhsiSj","colab":{"base_uri":"https://localhost:8080/","height":224},"executionInfo":{"status":"ok","timestamp":1655910476481,"user_tz":-120,"elapsed":4790,"user":{"displayName":"Nico T","userId":"09615745923254077330"}},"outputId":"d5be8993-f0d2-41f7-e8e4-c1feca3fab4a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]},{"output_type":"execute_result","data":{"text/plain":["                        date                     from  \\\n","0  2001-05-14 16:39:00-07:00  phillip.allen@enron.com   \n","1  2001-05-04 13:51:00-07:00  phillip.allen@enron.com   \n","2  2000-10-18 03:00:00-07:00  phillip.allen@enron.com   \n","3  2000-10-23 06:13:00-07:00  phillip.allen@enron.com   \n","4  2000-08-31 05:07:00-07:00  phillip.allen@enron.com   \n","\n","                        to     header  \\\n","0     tim.belden@enron.com        NaN   \n","1  john.lavorato@enron.com        Re:   \n","2   leah.arsdall@enron.com   Re: test   \n","3    randall.gay@enron.com        NaN   \n","4     greg.piper@enron.com  Re: Hello   \n","\n","                                                body  \n","0              ['', 'Here is our forecast', '', ' ']  \n","1  ['', 'Traveling to have a business meeting tak...  \n","2             ['', 'test successful.  way to go!!!']  \n","3  ['', 'Randy,', '', ' Can you send me a schedul...  \n","4        ['', \"Let's shoot for Tuesday at 11:45.  \"]  "],"text/html":["\n","  <div id=\"df-a8a30ce0-63c0-4086-9e96-3d4787916c98\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>date</th>\n","      <th>from</th>\n","      <th>to</th>\n","      <th>header</th>\n","      <th>body</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2001-05-14 16:39:00-07:00</td>\n","      <td>phillip.allen@enron.com</td>\n","      <td>tim.belden@enron.com</td>\n","      <td>NaN</td>\n","      <td>['', 'Here is our forecast', '', ' ']</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2001-05-04 13:51:00-07:00</td>\n","      <td>phillip.allen@enron.com</td>\n","      <td>john.lavorato@enron.com</td>\n","      <td>Re:</td>\n","      <td>['', 'Traveling to have a business meeting tak...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2000-10-18 03:00:00-07:00</td>\n","      <td>phillip.allen@enron.com</td>\n","      <td>leah.arsdall@enron.com</td>\n","      <td>Re: test</td>\n","      <td>['', 'test successful.  way to go!!!']</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2000-10-23 06:13:00-07:00</td>\n","      <td>phillip.allen@enron.com</td>\n","      <td>randall.gay@enron.com</td>\n","      <td>NaN</td>\n","      <td>['', 'Randy,', '', ' Can you send me a schedul...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2000-08-31 05:07:00-07:00</td>\n","      <td>phillip.allen@enron.com</td>\n","      <td>greg.piper@enron.com</td>\n","      <td>Re: Hello</td>\n","      <td>['', \"Let's shoot for Tuesday at 11:45.  \"]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8a30ce0-63c0-4086-9e96-3d4787916c98')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a8a30ce0-63c0-4086-9e96-3d4787916c98 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a8a30ce0-63c0-4086-9e96-3d4787916c98');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":1}],"source":["# Mount GDrive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","import pandas as pd\n","# Pour limiter les temps de calcul on travaille uniquement sur les 'nrows' premiers mails\n","df = pd.read_csv(\"/content/drive/MyDrive/Datascientest/Projet PY_email Datascientest/Data/Enron Cleaned Data/enron_unique.csv\", nrows=1000)\n","df.head()"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"7uLPAsx-tPfL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910476482,"user_tz":-120,"elapsed":20,"user":{"displayName":"Nico T","userId":"09615745923254077330"}},"outputId":"b33b42eb-ad55-4c77-d182-e0c8c3c7bd1b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["149"]},"metadata":{},"execution_count":2}],"source":["# nbre d'expéditeurs distincts\n","len(df['from'].unique())"]},{"cell_type":"code","source":["# nbre de destinataires distincts\n","len(df['to'].unique())"],"metadata":{"id":"9itN2Crz6yKN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910476482,"user_tz":-120,"elapsed":16,"user":{"displayName":"Nico T","userId":"09615745923254077330"}},"outputId":"90892b8b-1f6f-4a9e-df38-23d0392d6462"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["231"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"geS2lCMeIpwf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910476483,"user_tz":-120,"elapsed":15,"user":{"displayName":"Nico T","userId":"09615745923254077330"}},"outputId":"21c7bcf8-1ce7-4e8c-fd1e-3b606b74f708"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1000 entries, 0 to 999\n","Data columns (total 9 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   date        1000 non-null   object\n"," 1   from        1000 non-null   object\n"," 2   to          1000 non-null   object\n"," 3   header      784 non-null    object\n"," 4   body        1000 non-null   object\n"," 5   NER_header  1000 non-null   object\n"," 6   NER_body    1000 non-null   object\n"," 7   body_dict   1000 non-null   object\n"," 8   summary     1000 non-null   object\n","dtypes: object(9)\n","memory usage: 70.4+ KB\n"]}],"source":["import numpy as np\n","df = df.replace('NaN', np.NaN)\n","# df_emails = df[df['header'].notna()]\n","df_emails = df\n","df_emails['NER_header'] = ''\n","df_emails['NER_body'] = ''\n","df_emails['body_dict'] = ''\n","df_emails['summary'] = ''\n","df_emails.info()\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"QPpuAqcYhyLB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910476484,"user_tz":-120,"elapsed":14,"user":{"displayName":"Nico T","userId":"09615745923254077330"}},"outputId":"74a02ebb-45e7-4afe-8e2b-59e4f2bc3b35"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['date', 'from', 'to', 'header', 'body', 'NER_header', 'NER_body',\n","       'body_dict', 'summary'],\n","      dtype='object')"]},"metadata":{},"execution_count":5}],"source":["df_emails.columns"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"yh4-upAdXbSS","executionInfo":{"status":"ok","timestamp":1655910476485,"user_tz":-120,"elapsed":12,"user":{"displayName":"Nico T","userId":"09615745923254077330"}}},"outputs":[],"source":["#\n","# A ce stade le fichier a la structure nécessaire à Melusine pour le nettoyage\n","#"]},{"cell_type":"code","source":["# !pip install -U spacy\n","#!python -m spacy download en_core_web_sm\n","\n","\n"],"metadata":{"id":"FEbHex0arHC_","executionInfo":{"status":"ok","timestamp":1655910476485,"user_tz":-120,"elapsed":11,"user":{"displayName":"Nico T","userId":"09615745923254077330"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["import spacy\n","from spacy import displacy\n","nlp = spacy.load('en_core_web_sm')\n","spacy_stopwords = nlp.Defaults.stop_words\n","stopwords = spacy_stopwords\n","print(spacy_stopwords)"],"metadata":{"id":"sHTuVMN4iZaQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910480832,"user_tz":-120,"elapsed":4357,"user":{"displayName":"Nico T","userId":"09615745923254077330"}},"outputId":"1cbab19c-0afb-4e8e-d35d-972148a0c6f8"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["{\"'s\", 'a', 'anywhere', 'been', 'across', 'former', 'whenever', 'over', 'off', 'on', 'whence', 'made', 'to', 'very', 'can', 'somehow', 'everywhere', 'third', 'full', 'here', 'once', 'call', 'might', 'regarding', '‘ve', 'against', 'several', 'two', 'us', 'your', 'many', 'no', 'sixty', 'still', 'indeed', 'among', 'nevertheless', 'such', 'seeming', 'whose', 'see', 'nobody', 'every', 'would', \"'ve\", 'almost', 'because', 'least', 'before', 'formerly', 'doing', 'afterwards', 'further', 'hereafter', 'up', 'mostly', 'those', 'latter', 'whole', \"n't\", '’m', 'with', 'than', 'most', 'anything', 'him', 'through', 'as', 'ours', 'elsewhere', 'these', 'something', 'there', 'twenty', 'next', 'had', 'though', 'fifteen', 'neither', 'therefore', 'herein', 'although', 'hence', 'should', 'my', 'unless', 'done', 'along', 'after', 'both', 'otherwise', 'besides', 'onto', 'i', 'ever', 'yours', 'more', 'become', 'less', 'twelve', 'five', 'per', 'some', \"'d\", 'it', 'seem', 'below', 'by', \"'re\", 'our', 'they', 'give', 'beyond', 'toward', 'nowhere', 'few', 'towards', 'various', 'also', 'throughout', 'mine', 'seemed', '‘d', 'other', 'namely', 'anyone', 'take', 'meanwhile', 'thence', 'how', 'together', 'bottom', 'herself', 'whoever', 'never', 'go', 'behind', 'seems', 'in', 'three', 'between', 'hundred', 'sometimes', 'thus', 'above', 'via', 'where', 'without', \"'m\", 'noone', 'its', 'upon', '‘ll', 'everything', 'am', 'yourself', 'even', 'whom', 'nine', 'while', 'now', 'being', 'of', '‘s', 'whereas', 'really', 'which', 'already', 'sometime', 're', 'just', 'any', 'anyway', 'top', 'thereupon', 'from', 'ourselves', 'own', 'amount', 'else', 'around', 'again', 'nor', 'therein', 'but', 'he', 'whatever', 'itself', 'she', 'however', 'an', 'yet', 'always', 'have', '’re', 'n’t', 'wherever', 'n‘t', 'one', 'does', 'same', 'out', 'everyone', 'each', 'moreover', 'forty', 'beside', '’ll', 'their', 'quite', 'six', 'only', 'me', 'ca', 'name', 'front', 'so', 'show', 'please', 'are', '’s', 'under', 'enough', 'eleven', 'others', 'this', 'hereupon', 'into', 'keep', 'when', 'may', 'why', '‘re', 'alone', 'thru', 'part', 'whether', 'hereby', 'you', 'becoming', 'down', 'her', 'another', 'put', 'fifty', 'or', 'serious', 'latterly', 'perhaps', 'move', 'we', \"'ll\", 'the', 'used', '’d', 'often', 'themselves', 'be', 'whither', 'then', 'cannot', 'much', 'during', 'somewhere', 'for', '‘m', 'and', 'do', 'wherein', 'none', 'became', 'that', 'all', 'say', 'hers', 'last', 'four', 'since', 'himself', 'myself', 'first', 'nothing', 'at', 'not', 'is', 'them', 'yourselves', 'amongst', 'will', 'eight', 'using', 'whereafter', 'who', 'make', 'was', 'until', 'has', 'ten', 'could', 'empty', 'either', '’ve', 'did', 'someone', 'rather', 'whereupon', 'within', 'beforehand', 'anyhow', 'about', 'except', 'becomes', 'his', 'if', 'must', 'due', 'well', 'too', 'were', 'whereby', 'side', 'back', 'get', 'thereby', 'what', 'thereafter'}\n"]}]},{"cell_type":"code","source":["import nltk\n","# nltk.download('punkt')\n","from nltk.tokenize import sent_tokenize\n","import re\n","\n","def untokenize (tokens):\n","# Suppression des caractres spéciaux ou inutiles \\\"',]\n","  r = re.compile(r\"[\\\\\\''\\\",]\")\n","  texte=r.sub('', tokens)\n","  texte = sent_tokenize(texte)\n","#  print(\"Après :\\n\",type(texte),\" \",texte)\n","  return texte"],"metadata":{"id":"J3AM6gXAjKn2","executionInfo":{"status":"ok","timestamp":1655910480833,"user_tz":-120,"elapsed":9,"user":{"displayName":"Nico T","userId":"09615745923254077330"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def remove_stop_words(text,stopwords):\n","  lst=[]\n","  for token in str(text).split():\n","    if token.lower() not in stopwords:    #checking whether the word is not \n","      lst.append(token)                    #present in the stopword list.      \n","  # Join items in the list\n","#  print(\"Original text  : \",text)\n","  result = ' '.join(lst)\n","#  print(\"Text after removing stopwords  :   \",result)\n","  return result"],"metadata":{"id":"S_I3qqRIt79M","executionInfo":{"status":"ok","timestamp":1655910480834,"user_tz":-120,"elapsed":9,"user":{"displayName":"Nico T","userId":"09615745923254077330"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Transforme Text en Dictionnaire - Key = indice de phrase - Value = Mots de la phrase\n","import re\n","from nltk.tokenize import sent_tokenize,word_tokenize\n","\n","def dictionarize(article):\n","    dico={}\n","    phrases=sent_tokenize(article)\n","    phr2=[]\n","    for i, phr in enumerate(phrases):\n","        import re\n","        phr=re.sub(pattern =\"[^a-zA-Z]\", repl = ' ', string = phr)\n","        phr2.append(phr)\n","    \n","        phr=word_tokenize(phr)\n","        phr_lower=[w.lower() for w in phr]\n","        if len(phr_lower)>4:      \n","            dico[i]=phr_lower    \n","    return dico\n","\n"],"metadata":{"id":"v7Enu2jNVjHU","executionInfo":{"status":"ok","timestamp":1655910480834,"user_tz":-120,"elapsed":8,"user":{"displayName":"Nico T","userId":"09615745923254077330"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["df_emails.shape[0]"],"metadata":{"id":"YdwAE_vc_o5o","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655910480834,"user_tz":-120,"elapsed":8,"user":{"displayName":"Nico T","userId":"09615745923254077330"}},"outputId":"c1070c35-6a0e-45fb-aeac-806c0a4be819"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1000"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["# test unitaire de la fonction dictionarize sur une ligne\n","#\n","# artikle = dictionarize(df_emails['body'][32])\n","# for cle, valeur in artikle.items():\n","#        print(\"clé\", cle, \"vaut\", valeur)"],"metadata":{"id":"uNzCe6vWHyC2","executionInfo":{"status":"ok","timestamp":1655910480835,"user_tz":-120,"elapsed":7,"user":{"displayName":"Nico T","userId":"09615745923254077330"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","\n","def TF(token, artikle):\n","    \"\"\"\n","    Calcule le score TF d'un mot dans un artikle\n","    \n","    token : Mot dont le score TF doit être calculé.\n","    \n","    artikle : Dictionnaire généré à partir d'un texte.\n","    \"\"\"\n","    f = 0\n","    for key in artikle:\n","        for work in artikle[key]:\n","            if work == token:\n","                f += 1 \n","    return np.log(f+1)\n","\n","def IDF(token, corpus):\n","    \"\"\"\n","    Calcule le score IDF d'un mot dans un corpus d'artikles.\n","    \n","    token : Mot dont le score IDF doit être calculé.\n","    \n","    corpus : Liste d'artikles.\n","    \"\"\"\n","    N = len(corpus)\n","    d=0\n","    present = False\n","    \n","    for artikle in corpus:\n","        for key in artikle:\n","            if token in artikle[key]:\n","                present = True\n","        d += int(present)\n","        present = False\n","                \n","    return np.log(N/(d+1) +1)\n","\n","def TFIDF(token, artikle, corpus):\n","    \"\"\"\n","    Calcule le score TF-IDF d'un mot dans un texte.\n","    \n","    token : mot dont le score doit être calculé.\n","    \n","    artikle : artikle qui servira à calculer le score du mot dans cet artikle.\n","    \n","    corpus : Liste d'artikles formant le corpus.\n","    \"\"\"\n","    return TF(token, artikle)*IDF(token, corpus)\n","\n","def score_sentence(corpus, artikle, sentence):\n","    \"\"\"\n","    Calcule le score d'une phrase.\n","    \n","    corpus : Liste d'artikles.\n","    \n","    artikle : Dictionnaire de phrases.\n","    \n","    sentence : Phrase sous forme de liste de mots.\n","    \"\"\"\n","    score_sentence = []\n","    for word in sentence :\n","        score_word = TFIDF(word, artikle, corpus)\n","        score_sentence.append(score_word)\n","    return np.mean(score_sentence)\n","\n","def best_sentences(scores_artikle, nb_sentences):\n","    \"\"\"\n","    Retourne les indices des phrases les plus importantes en fonction des scores obtenus.\n","    \n","    scores_artikle : Liste des scores de chaque phrase dans un texte.\n","    \n","    nb_sentences : Nombre de phrases à sélectionner.\n","    \"\"\"\n","    \n","    return sorted(np.argsort(scores_artikle)[-nb_sentences:])\n","\n","def summarize(i, n_sentences, df):\n","    \"\"\"\n","    Synthèse extractive d'un article par la méthode TF-IDF.\n","    \n","    i : indice de l'article dans le corpus.\n","    \n","    n_sentences : nombre de phrases à conserver.\n","    \n","    df : DataFrame contenant les artikles dans une colonne 'Artikle'.    \n","    \"\"\"\n","    corpus = df['body_dict']\n","    artikle = corpus[i]\n","    texte=df['body'][i]\n","    \n","    \n","    # Calcul du score de chaque phrase de l'artikle\n","    scores_artikle = [score_sentence(corpus, artikle, sentence) for sentence in artikle.values()]\n"," \n","    \n","    # Extraction des indices des phrases ayant les meilleurs scores\n","    result = best_sentences(scores_artikle, n_sentences)\n","#    print('best sentences :',result)\n","    \n","    # Séparation de phrases l'article original \n","    tokenized_article = sent_tokenize(texte)\n","    \n","    # Extraction des phrases les plus importantes\n","    summary = [tokenized_article[i] for i in result]\n","#    print(summary)\n","    return summary"],"metadata":{"id":"E_flKIgS5ra2","executionInfo":{"status":"ok","timestamp":1655910481364,"user_tz":-120,"elapsed":536,"user":{"displayName":"Nico T","userId":"09615745923254077330"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Nettoyage : Détokenisation et retokenisation en sentences\n","deb = 0\n","fin = df_emails.shape[0]\n","# fin = 100\n","for id in range(deb,fin):\n","#  print(\"[\",id,\" :\")\n","  text = df_emails['body'].iloc[id]\n","  text = untokenize(text)\n","#  text = remove_stop_words(text,stopwords)\n","  text = dictionarize(str(text))\n","  df_emails['body_dict'][id] = text\n","# résumé dans le champ 'summarize' en 3 lignes au plus.\n","  df_emails['summary'][id] = summarize(id, 3, df_emails)\n","#  print(\"apres remove stopwords() : \",df_emails['body_dict'][id])\n"],"metadata":{"id":"dQ8G9SzEFtFL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_emails['body'][32]"],"metadata":{"id":"h8ha9jRhXy_C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_emails['summary'][32]"],"metadata":{"id":"gs0vAM2z8M1V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"4fpiwZTG8NCe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# texte fonction de résumé sur la ligne 'id'\n","id = 12\n","#print(df_emails['body'][id])\n","for cle, valeur in df_emails['body_dict'][id].items():\n","        print(\"Ligne \", cle, \"vaut\", valeur)\n","\n","\n","print('summary :\\n\\n',summarize(id, 3, df_emails))\n","\n","# df_emails['summary'].iloc[id] = summarize(id, 5, df_emails)\n","# print(df_emails['summary'][id])"],"metadata":{"id":"Jf30ZxO65sSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IjvFt1AbRNBS"},"source":["# SPACY - NER"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PYcF03C2v-Pt"},"outputs":[],"source":["deb = 0\n","fin = df_emails.shape[0]\n","for x in range(deb,fin):\n","# subject\n","  subj = df_emails.iloc[x]['header']\n","#  print(x,' - ',subj)\n","  doc_subj = nlp(str(subj))\n","  df_emails['NER_header'][x] = doc_subj.ents\n","\n","# body\n","  subj = df_emails.iloc[x]['body']\n","#  print(x,' - ',subj)\n","  doc_subj = nlp(str(subj))\n","  df_emails['NER_body'][x] = doc_subj.ents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NyAzFKn7X2uO"},"outputs":[],"source":["df_emails[['header','NER_header','NER_body']][20:45]\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"23GUNvjEcvjP"},"outputs":[],"source":["id = 44\n","doc= nlp(df_emails['header'][id])\n","displacy.render(doc, style = \"ent\",jupyter = True)\n","print(\"----- eMail \",id,\" -----\")\n","doc= nlp(df_emails['body'][id])\n","displacy.render(doc, style = \"ent\",jupyter = True)\n","print(\"\\n\")\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LgS5AAxWhsnK"},"outputs":[],"source":["spacy.explain(\"PERSON\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P6Px81DsaBEj"},"outputs":[],"source":["# A ce stade le fichier a la structure attendue par melusine + l'identification des NER par Spacy\n","df_emails[['header','NER_header','body','NER_body']].iloc[id]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HjkOEh1BlpSw"},"outputs":[],"source":["df_emails['body'].iloc[id]"]},{"cell_type":"markdown","source":["# Summary"],"metadata":{"id":"gScc6J-L5m6L"}},{"cell_type":"code","source":["df_emails[['body','body_dict']]"],"metadata":{"id":"DaUrJQ7lDqlq"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["IjvFt1AbRNBS"],"name":"Py_Easymail.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}